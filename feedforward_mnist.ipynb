{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def setup(self):\n",
    "        self.layer1 = nn.Dense(784)\n",
    "        self.layer2 = nn.Dense(10)\n",
    "\n",
    "    def __call__(self, x, deterministic=False):\n",
    "        out = self.layer1(x)\n",
    "        out = nn.relu(out)\n",
    "        return self.layer2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using PyTorch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root=\"/home/tejasj/data\", train=True, download=True)\n",
    "\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        return {\n",
    "            \"image\": torchvision.transforms.ToTensor()(data[0]).cpu().numpy(),\n",
    "            \"label\": np.array(data[1]).reshape(-1, ),\n",
    "        }\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=MNISTDataset(),\n",
    "    batch_size=num_devices * batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of visible devices\n",
    "num_devices = jax.device_count()\n",
    "\n",
    "# Seed the random number generator\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize the model\n",
    "classifier = Classifier()\n",
    "key, sub_key = jax.random.split(key)\n",
    "\n",
    "# This returns all the parameters in a frozen dict\n",
    "variables = classifier.init(key, jnp.ones((1, 784)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "from clu import metrics\n",
    "from flax import struct\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output(\"loss\")\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "\n",
    "tx = optax.adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "train_state = TrainState(\n",
    "    step=0,\n",
    "    apply_fn=classifier.apply,\n",
    "    params=variables[\"params\"],\n",
    "    tx=tx,\n",
    "    opt_state=tx.init(variables[\"params\"]),\n",
    "    metrics=Metrics.empty(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.jax_utils as flax_utils\n",
    "\n",
    "# Initialize the train step\n",
    "def loss_fn(params, batch):\n",
    "    output = train_state.apply_fn(\n",
    "        {\"params\": params},\n",
    "        batch[\"image\"],\n",
    "    )\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=output, labels=batch[\"label\"]\n",
    "    ).mean()\n",
    "    return loss, output\n",
    "\n",
    "\n",
    "def train_step(state, batch):\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params, batch)\n",
    "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
    "\n",
    "    new_state = state.apply_gradients(\n",
    "        grads=grads,\n",
    "    )\n",
    "    metric_updates = state.metrics.gather_from_model_output(\n",
    "        loss=loss, logits=logits, labels=batch[\"label\"]\n",
    "    )\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    new_state = new_state.replace(metrics=metrics)\n",
    "\n",
    "    return new_state\n",
    "\n",
    "\n",
    "# Do some initialization for parallel training\n",
    "p_train_step = jax.pmap(train_step, axis_name=\"batch\")\n",
    "\n",
    "\n",
    "def train(start_state):\n",
    "    state = start_state\n",
    "\n",
    "    # Distribute training\n",
    "    state = flax_utils.replicate(state)\n",
    "\n",
    "    for _ in range(5):\n",
    "        for data in dataloader:\n",
    "            num_samples = data[\"image\"].shape[0]\n",
    "            batch = {\n",
    "                \"image\": data[\"image\"].permute(0, 2, 3, 1).reshape(num_devices, num_samples // num_devices, -1).numpy(),\n",
    "                \"label\": data[\"label\"].reshape(num_devices, num_samples // num_devices, ).numpy(),\n",
    "            }\n",
    "            state = p_train_step(state, batch)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train(train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': Array(0.9751533, dtype=float32), 'loss': Array(0.08414858, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(state.metrics.unreplicate().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_params(key, state):\n",
    "    def add_noise(param):\n",
    "        new_param = param + 0.01 * jax.random.uniform(key, param.shape)\n",
    "        return new_param\n",
    "    new_params = jax.tree_map(add_noise, state.params)\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_params_multiple(state, num_params):\n",
    "    key = jax.random.PRNGKey(42)\n",
    "\n",
    "    new_params = []\n",
    "    for _ in range(num_params):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        new_param = add_noise_to_params(subkey, state)\n",
    "        new_params.append(new_param)\n",
    "    \n",
    "    return new_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_unrpl = flax_utils.unreplicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_states = add_noise_to_params_multiple(state_unrpl, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:            \n",
    "    num_samples = data[\"image\"].shape[0]\n",
    "    batch = {\n",
    "        \"image\": data[\"image\"].permute(0, 2, 3, 1).reshape(num_samples, -1).numpy(),\n",
    "        \"label\": data[\"label\"].reshape(num_samples, ).numpy(),\n",
    "    }\n",
    "    break\n",
    "\n",
    "def loss_fn_svd(params):\n",
    "    logits = classifier.apply({\"params\": params}, batch[\"image\"])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch[\"label\"]\n",
    "    ).mean()\n",
    "    return loss\n",
    "\n",
    "loss_fn_svd_grad = jax.grad(loss_fn_svd)\n",
    "\n",
    "grads = []\n",
    "for params in new_states:\n",
    "    grads.append(loss_fn_svd_grad(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = []\n",
    "for params in grads:\n",
    "    leaves.append(jax.lax.concatenate(jax.tree_map(lambda x: x.reshape(-1, 1), jax.tree_util.tree_leaves(params)), dimension=0))\n",
    "leaves = jax.lax.concatenate(leaves, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(623290, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "    U, _, _ = jnp.linalg.svd(leaves, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 18:23:34.526093: W external/xla/xla/service/hlo_rematerialization.cc:2218] Can't reduce memory use below 17.77GiB (19078594560 bytes) by rematerialization; only reduced to 1.41TiB (1553961696400 bytes)\n",
      "2023-07-16 18:23:44.591465: W external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.41TiB (rounded to 1553961696512)requested by op \n",
      "2023-07-16 18:23:44.591668: W external/tsl/tsl/framework/bfc_allocator.cc:497] *___________________________________________________________________________________________________\n",
      "2023-07-16 18:23:44.591764: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2461] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1553961696400 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:         0B\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:    1.41TiB\n",
      "     preallocated temp allocation:         0B\n",
      "                 total allocation:    1.41TiB\n",
      "              total fragmentation:         0B (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 1.41TiB\n",
      "\t\tOperator: op_name=\"jit(iota)/jit(main)/iota[dtype=int32 shape=(623290, 623290) dimension=0]\" source_file=\"/tmp/ipykernel_3307486/406865854.py\" source_line=20\n",
      "\t\tXLA Label: iota\n",
      "\t\tShape: s32[623290,623290]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1553961696400 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:         0B\n              constant allocation:         0B\n        maybe_live_out allocation:    1.41TiB\n     preallocated temp allocation:         0B\n                 total allocation:    1.41TiB\n              total fragmentation:         0B (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 1.41TiB\n\t\tOperator: op_name=\"jit(iota)/jit(main)/iota[dtype=int32 shape=(623290, 623290) dimension=0]\" source_file=\"/tmp/ipykernel_3307486/406865854.py\" source_line=20\n\t\tXLA Label: iota\n\t\tShape: s32[623290,623290]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m grads \u001b[39m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m params \u001b[39min\u001b[39;00m new_states:\n\u001b[0;32m---> 20\u001b[0m     grads\u001b[39m.\u001b[39mappend(loss_fn_svd_hess(params))\n\u001b[1;32m     21\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/api.py:842\u001b[0m, in \u001b[0;36mjacfwd.<locals>.jacfun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_aux:\n\u001b[1;32m    841\u001b[0m   pushfwd: Callable \u001b[39m=\u001b[39m partial(_jvp, f_partial, dyn_args)\n\u001b[0;32m--> 842\u001b[0m   y, jac \u001b[39m=\u001b[39m vmap(pushfwd, out_axes\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))(_std_basis(dyn_args))\n\u001b[1;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m   pushfwd: Callable \u001b[39m=\u001b[39m partial(_jvp, f_partial, dyn_args, has_aux\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/api.py:1022\u001b[0m, in \u001b[0;36m_std_basis\u001b[0;34m(pytree)\u001b[0m\n\u001b[1;32m   1020\u001b[0m ndim \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(np\u001b[39m.\u001b[39msize, leaves))\n\u001b[1;32m   1021\u001b[0m dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mresult_type(\u001b[39m*\u001b[39mleaves)\n\u001b[0;32m-> 1022\u001b[0m flat_basis \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49meye(ndim, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1023\u001b[0m \u001b[39mreturn\u001b[39;00m _unravel_array_into_pytree(pytree, \u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m, flat_basis)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:2272\u001b[0m, in \u001b[0;36meye\u001b[0;34m(N, M, k, dtype)\u001b[0m\n\u001b[1;32m   2270\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnegative dimensions are not allowed, got \u001b[39m\u001b[39m{\u001b[39;00mN\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mM\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2271\u001b[0m k \u001b[39m=\u001b[39m operator\u001b[39m.\u001b[39mindex(k)\n\u001b[0;32m-> 2272\u001b[0m \u001b[39mreturn\u001b[39;00m lax_internal\u001b[39m.\u001b[39;49m_eye(_jnp_dtype(dtype), (N_int, M_int), k)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/lax/lax.py:1240\u001b[0m, in \u001b[0;36m_eye\u001b[0;34m(dtype, shape, offset)\u001b[0m\n\u001b[1;32m   1238\u001b[0m offset \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(offset)\n\u001b[1;32m   1239\u001b[0m dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mcanonicalize_dtype(dtype)\n\u001b[0;32m-> 1240\u001b[0m bool_eye \u001b[39m=\u001b[39m eq(add(broadcasted_iota(np\u001b[39m.\u001b[39;49mint32, shape, \u001b[39m0\u001b[39;49m), np\u001b[39m.\u001b[39mint32(offset)),\n\u001b[1;32m   1241\u001b[0m               broadcasted_iota(np\u001b[39m.\u001b[39mint32, shape, \u001b[39m1\u001b[39m))\n\u001b[1;32m   1242\u001b[0m \u001b[39mreturn\u001b[39;00m convert_element_type_p\u001b[39m.\u001b[39mbind(bool_eye, new_dtype\u001b[39m=\u001b[39mdtype, weak_type\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/lax/lax.py:1233\u001b[0m, in \u001b[0;36mbroadcasted_iota\u001b[0;34m(dtype, shape, dimension)\u001b[0m\n\u001b[1;32m   1230\u001b[0m static_shape \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(d, core\u001b[39m.\u001b[39mTracer) \u001b[39melse\u001b[39;00m d \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m shape]\n\u001b[1;32m   1231\u001b[0m dimension \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mconcrete_or_error(\n\u001b[1;32m   1232\u001b[0m     \u001b[39mint\u001b[39m, dimension, \u001b[39m\"\u001b[39m\u001b[39mdimension argument of lax.broadcasted_iota\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1233\u001b[0m \u001b[39mreturn\u001b[39;00m iota_p\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49mdynamic_shape, dtype\u001b[39m=\u001b[39;49mdtype, shape\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(static_shape),\n\u001b[1;32m   1234\u001b[0m                    dimension\u001b[39m=\u001b[39;49mdimension)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    384\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/dispatch.py:144\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    140\u001b[0m   msg \u001b[39m=\u001b[39m pjit\u001b[39m.\u001b[39m_device_assignment_mismatch_error(\n\u001b[1;32m    141\u001b[0m       prim\u001b[39m.\u001b[39mname, fails, args, \u001b[39m'\u001b[39m\u001b[39mjit\u001b[39m\u001b[39m'\u001b[39m, arg_names)\n\u001b[1;32m    142\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fun(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/dispatch.py:227\u001b[0m, in \u001b[0;36mxla_primitive_callable.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    223\u001b[0m compiled \u001b[39m=\u001b[39m _xla_callable_uncached(\n\u001b[1;32m    224\u001b[0m     lu\u001b[39m.\u001b[39mwrap_init(prim_fun), prim\u001b[39m.\u001b[39mname, donated_invars, \u001b[39mFalse\u001b[39;00m, in_avals,\n\u001b[1;32m    225\u001b[0m     orig_in_shardings)\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prim\u001b[39m.\u001b[39mmultiple_results:\n\u001b[0;32m--> 227\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: compiled(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m   \u001b[39mreturn\u001b[39;00m compiled\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1349\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1344\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_token_bufs(\n\u001b[1;32m   1345\u001b[0m       results\u001b[39m.\u001b[39mdisassemble_prefix_into_single_device_arrays(\n\u001b[1;32m   1346\u001b[0m           \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mordered_effects)),\n\u001b[1;32m   1347\u001b[0m       results\u001b[39m.\u001b[39mconsume_token())\n\u001b[1;32m   1348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxla_executable\u001b[39m.\u001b[39;49mexecute_sharded(input_bufs)\n\u001b[1;32m   1350\u001b[0m \u001b[39mif\u001b[39;00m dispatch\u001b[39m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1351\u001b[0m   out_arrays \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1553961696400 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:         0B\n              constant allocation:         0B\n        maybe_live_out allocation:    1.41TiB\n     preallocated temp allocation:         0B\n                 total allocation:    1.41TiB\n              total fragmentation:         0B (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 1.41TiB\n\t\tOperator: op_name=\"jit(iota)/jit(main)/iota[dtype=int32 shape=(623290, 623290) dimension=0]\" source_file=\"/tmp/ipykernel_3307486/406865854.py\" source_line=20\n\t\tXLA Label: iota\n\t\tShape: s32[623290,623290]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:            \n",
    "    num_samples = data[\"image\"].shape[0]\n",
    "    batch = {\n",
    "        \"image\": data[\"image\"].permute(0, 2, 3, 1).reshape(num_samples, -1).numpy(),\n",
    "        \"label\": data[\"label\"].reshape(num_samples, ).numpy(),\n",
    "    }\n",
    "    break\n",
    "\n",
    "def loss_fn_svd(params):\n",
    "    logits = classifier.apply({\"params\": params}, batch[\"image\"])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch[\"label\"]\n",
    "    ).mean()\n",
    "    return loss\n",
    "\n",
    "loss_fn_svd_hess = jax.hessian(loss_fn_svd)\n",
    "\n",
    "grads = []\n",
    "for params in new_states:\n",
    "    grads.append(loss_fn_svd_hess(params))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
